"""Step 2: Chinese RAG evaluation with ground truth.

Tests the actual production configuration (Chinese FAQ + Chinese system prompt).
"""

import json
import sys
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

from kbee.config import settings

# Chinese test cases against the existing FAQ data in data/faq/zh_tw/
TEST_CASES = [
    # --- Direct match questions ---
    {
        "question": "Ë´ãÂïèÂ¶Ç‰ΩïÁî≥Ë´ãÈÄÄÊ¨æÔºü",
        "ground_truth": "Ë≥ºË≤∑Âæå 30 Â§©ÂÖßÂèØÁî≥Ë´ãÈÄÄÊ¨æ„ÄÇÂâçÂæÄ„ÄåÊàëÁöÑË®ÇÂñÆ„ÄçÈÅ∏ÊìáË®ÇÂñÆÔºåÈªûÊìä„ÄåÁî≥Ë´ãÈÄÄÊ¨æ„ÄçÔºåÈÄÄÊ¨æÂú® 5-7 ÂÄãÂ∑•‰ΩúÂ§©ÂÖßÈÄÄÂõûÂéü‰ªòÊ¨æÊñπÂºè„ÄÇ",
        "source_keywords": ["30 Â§©", "ÊàëÁöÑË®ÇÂñÆ", "Áî≥Ë´ãÈÄÄÊ¨æ"],
    },
    {
        "question": "‰Ω†ÂÄëÊúâÂì™‰∫õ‰ªòÊ¨æÊñπÂºèÔºü",
        "ground_truth": "ÊîØÊè¥‰ø°Áî®Âç°ÔºàVisa„ÄÅMasterCard„ÄÅJCBÔºâ„ÄÅÈäÄË°åËΩâÂ∏≥„ÄÅË∂ÖÂïÜ‰ªòÊ¨æ„ÄÅLINE Pay„ÄÅApple Pay Âíå Google Pay„ÄÇ",
        "source_keywords": ["Visa", "LINE Pay", "Apple Pay"],
    },
    {
        "question": "VIP ÊúÉÂì°Êúâ‰ªÄÈ∫ºÂÑ™ÊÉ†Ôºü",
        "ground_truth": "VIP ÊúÉÂì°‰∫´Êúâ 9 ÊäòÂÑ™ÊÉ†„ÄÅÂÖçÈÅãË≤ª„ÄÅÂ∞àÂ±¨ÂÆ¢ÊúçÈÄöÈÅì„ÄÇÂπ¥Ê∂àË≤ªÈÅî NT$10,000 Ëá™ÂãïÂçáÁ¥ö„ÄÇ",
        "source_keywords": ["9 Êäò", "ÂÖçÈÅãË≤ª", "10,000"],
    },
    # --- Paraphrased / colloquial questions ---
    {
        "question": "Êù±Ë•øÂ£û‰∫ÜÂèØ‰ª•ÊèõÂóéÔºü",
        "ground_truth": "Êî∂Âà∞ÁëïÁñµÂìÅ 7 Â§©ÂÖßËÅØÁπ´ÂÆ¢Êúç‰∏¶Êèê‰æõÁÖßÁâáÔºåÁ¢∫Ë™çÂæåÂÖçË≤ªÈÄÄÊèõË≤®ÔºåÈÅãË≤ªÁî±ÂÖ¨Âè∏ÊâøÊìî„ÄÇ",
        "source_keywords": ["ÁëïÁñµ", "7 Â§©", "ÁÖßÁâá"],
    },
    {
        "question": "ÊÄéÈ∫ºÊ®£ÊâçËÉΩËÆä VIPÔºü",
        "ground_truth": "Âπ¥Â∫¶Ê∂àË≤ªÈáëÈ°çÈÅîÂà∞ NT$10,000 ÊôÇÁ≥ªÁµ±Ëá™ÂãïÂçáÁ¥öÁÇ∫ VIP ÊúÉÂì°„ÄÇ",
        "source_keywords": ["10,000", "Ëá™ÂãïÂçáÁ¥ö"],
    },
    {
        "question": "ÊàëÊÉ≥ÂèñÊ∂àË®ÇÂñÆÊÄéÈ∫ºËæ¶Ôºü",
        "ground_truth": "Êú™Âá∫Ë≤®ÂèØÂú®„ÄåÊàëÁöÑË®ÇÂñÆ„ÄçÁõ¥Êé•ÂèñÊ∂à„ÄÇÂ∑≤Âá∫Ë≤®Ë´ãËÅØÁπ´ÂÆ¢ÊúçÂÆâÊéíÈÄÄË≤®„ÄÇÂèñÊ∂àÂæåÈÄÄÊ¨æ 3-5 ÂÄãÂ∑•‰ΩúÂ§©ËôïÁêÜ„ÄÇ",
        "source_keywords": ["ÂèñÊ∂à", "ÊàëÁöÑË®ÇÂñÆ", "3-5"],
    },
    # --- Multi-intent / complex questions ---
    {
        "question": "ÊàëÊÉ≥ÈÄÄÊ¨æÈ†Ü‰æøÂïè‰∏Ä‰∏ãÈÅãË≤ªÂ§öÂ∞ëÔºü",
        "ground_truth": "ÈÄÄÊ¨æÔºöË≥ºË≤∑Âæå 30 Â§©ÂÖßÁî≥Ë´ã„ÄÇÈÖçÈÄÅÔºöÊ®ôÊ∫ñÈÖçÈÄÅ 3-5 Â§©ÔºåÂø´ÈÄüÈÖçÈÄÅ 1-2 Â§©„ÄÇ",
        "source_keywords": ["30 Â§©", "ÈÖçÈÄÅ"],
    },
    {
        "question": "Á©çÂàÜÊÄéÈ∫ºÁî®ÔºüVIP ÊúâÈ°çÂ§ñÁ©çÂàÜÂóéÔºü",
        "ground_truth": "ÊØèÊ∂àË≤ª NT$1 Á¥ØÁ©ç 1 ÈªûÔºå100 ÈªûÊäòÊäµ NT$1ÔºåÊúâÊïàÊúü 1 Âπ¥„ÄÇVIP ‰∫´ÈõôÂÄçÁ©çÂàÜ„ÄÇ",
        "source_keywords": ["Á©çÂàÜ", "100 Èªû", "ÈõôÂÄç"],
    },
    # --- Edge cases ---
    {
        "question": "ÂèØ‰ª•ÂØÑÂà∞Ê≥ïÂúãÂóéÔºü",
        "ground_truth": "ÁõÆÂâçÊîØÊè¥ÈÖçÈÄÅËá≥Êó•Êú¨„ÄÅÈüìÂúã„ÄÅÈ¶ôÊ∏Ø„ÄÅÊñ∞Âä†Âù°ÂíåÁæéÂúãÔºåÊú™ÊèêÂèäÊ≥ïÂúã„ÄÇ",
        "source_keywords": ["Êó•Êú¨", "ÁæéÂúã"],
    },
    {
        "question": "‰Ω†ÂÄëÁöÑ CEO ÊòØË™∞Ôºü",
        "ground_truth": "Áü•Ë≠òÂ∫´‰∏≠Ê≤íÊúâÈÄôÊñπÈù¢ÁöÑË≥áË®ä„ÄÇ",
        "source_keywords": [],
    },
    # --- Typo / informal ---
    {
        "question": "ÂØÜÁ¢ºÂøò‰∫ÜÊÄéËæ¶",
        "ground_truth": "ÂâçÂæÄË®≠ÂÆöÈ†ÅÈù¢ > ÂÆâÂÖ®Ë®≠ÂÆö > Êõ¥ÊîπÂØÜÁ¢º„ÄÇÈúÄËº∏ÂÖ•ÁõÆÂâçÂØÜÁ¢ºÂíåÊñ∞ÂØÜÁ¢ºÔºåÂª∫Ë≠∞Ëá≥Â∞ë 8 ÂÄãÂ≠óÂÖÉÂê´Â§ßÂ∞èÂØ´ÂíåÊï∏Â≠ó„ÄÇ",
        "source_keywords": ["ÂÆâÂÖ®Ë®≠ÂÆö", "Êõ¥ÊîπÂØÜÁ¢º"],
    },
    {
        "question": "APP Âì™Ë£°‰∏ãËºâÔºü",
        "ground_truth": "Âú® App Store Êàñ Google Play ÊêúÂ∞ãÂìÅÁâåÂêçÁ®±‰∏ãËºâ„ÄÇÈ¶ñÊ¨°‰∏ãËºâÂèØÁç≤ NT$100 ÊäòÊâ£Âà∏„ÄÇ",
        "source_keywords": ["App Store", "Google Play", "100"],
    },
]


def run_evaluation():
    """Run Step 2 Chinese evaluation."""
    from kbee.query import get_query_engine

    print("=" * 60)
    print("STEP 2: Chinese RAG Evaluation")
    print("=" * 60)

    # Use the existing Chinese FAQ data (already ingested)
    engine = get_query_engine()

    results = []
    retrieval_scores = []
    
    for i, tc in enumerate(TEST_CASES):
        q = tc["question"]
        response = engine.query(q)

        contexts = []
        if hasattr(response, "source_nodes"):
            contexts = [node.text for node in response.source_nodes]

        # Keyword retrieval check
        all_context = " ".join(contexts)
        kw_total = len(tc["source_keywords"])
        if kw_total > 0:
            kw_found = sum(1 for kw in tc["source_keywords"] if kw in all_context)
            score = kw_found / kw_total
        else:
            kw_found = 0
            score = 1.0
        retrieval_scores.append(score)

        result = {
            "question": q,
            "answer": str(response),
            "ground_truth": tc["ground_truth"],
            "contexts": contexts,
            "retrieval_score": score,
        }
        results.append(result)

        status = "‚úÖ" if score == 1.0 else ("üü°" if score >= 0.5 else "‚ùå")
        print(f"\n--- Q{i+1}: {q}")
        print(f"  Answer: {str(response)[:200]}...")
        print(f"  Retrieval: {status} ({kw_found}/{kw_total} keywords)")

    # Summary
    print("\n" + "=" * 60)
    print("SUMMARY ‚Äî Chinese RAG Evaluation")
    print("=" * 60)

    avg_retrieval = sum(retrieval_scores) / len(retrieval_scores)
    perfect = sum(1 for s in retrieval_scores if s == 1.0)
    partial = sum(1 for s in retrieval_scores if 0 < s < 1.0)
    failed = sum(1 for s in retrieval_scores if s == 0)

    print(f"Total test cases: {len(TEST_CASES)}")
    print(f"Avg retrieval score: {avg_retrieval:.1%}")
    print(f"Perfect retrieval: {perfect}/{len(TEST_CASES)}")
    print(f"Partial retrieval: {partial}/{len(TEST_CASES)}")
    print(f"Failed retrieval: {failed}/{len(TEST_CASES)}")

    # Save
    output_path = Path("eval/results_step2_zh.json")
    with open(output_path, "w") as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    print(f"\nDetailed results saved to: {output_path}")

    return results


if __name__ == "__main__":
    run_evaluation()
